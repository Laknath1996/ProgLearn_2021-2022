# 9/23/21 meeting notes

## MORF, manifold forests 
- it is somewhat like convolution
- there is a paper on this from the lab

- can push task 1 data through task 2 to learn task 1 posterior, then we can average them 
- learning structures are not free, you need data to learn them
- two partitions will help each other

## what do we mean by task
- essential components of task

![image](https://user-images.githubusercontent.com/89429238/134557890-8bfbfbda-ba60-491a-ab19-0335083eb130.png)

- task tries to finds an algorithm that learns a hypothesis that min's the risks
- given Q, A, D from true and unknown dist, the task T is to learn an algo from the algo space that finds a hypotheses H for the hypothesis space that minimizes risk
- in lifelong learning, your dataset can be streaming, comes in 1 by 1 and not in batches
- but for simplicity we will consider data coming in batches
- **task agnostic**: where you do not know which data set belong to which task
- **task adveristy**: you know which data set comes from which task

## what is learning?
- f learns from data Z_n (n is sample size) w/ respect to task t when its performance at t improves due to Z_n
- Define generalization error
- Z_0 correcponds to no data
- learning efficiency: 

![image](https://user-images.githubusercontent.com/89429238/134558830-b7e55298-35f3-4ce5-9a4d-82c15f65e250.png)

- if it learns something generalization error will decrease
- f learns from Z_n with respect to task t when LE_n(f) > 1

## what is forward learning
- you can have the same task repeatedly
- life long learning is suppose to make your learning more realistic (ie robot faces same task mult times)
- n_t last occurence of task t in Z_n
- Z_n\raisedto<t contains all the data up to t
                 
![image](https://user-images.githubusercontent.com/89429238/134559618-ecb47a5f-b02b-416f-bba1-1ea264c889af.png)

- when you intro new task, how do you perform with all the old data, want ratio >1
                 
## backward learning 
  
![image](https://user-images.githubusercontent.com/89429238/134559940-ca743e9c-95d9-4136-a876-21f8c3f6773f.png)

- f backward learns if BL_n(f) > 1                 

- it you multiply backward learning with forward learning, you get transfer learning. when you do this, time dissapears and it doesnt matter when you introduced the tasks

## for tutorials
- decrease number of trees and repetitions 
- going through tutorials understand how FTE, BTE stuff is calculated  
- go in order the tutorials are presented in the omni paper **not gaussian** 
- create pr with all the standardized notebooks 
  
