# NDD Meeting notes from September 9, 2021

Jayanta: 
Focus on generalization error

**GAUSSIAN XOR**

- 5 class chance error = 20% (.20)

- At each node there is a posterior

- if you overfit your model you'll do bad and your generalization error will be higher
- to fix this: have some priors so you dont split until a certain threshold, or can have multiple tree that overfit and when you avg them they cancel out each other

- Stone's theorem: in leaf nodes we have uniform distribution

- Posteriors are always uniform because any sample in that box will be assigned to that node

- To try to accurately approx the distribution, we can have smaller boxes and enough sample points in those boxes

- Only perform at bayes optimal when you have learned the whole distribution

- **Consistant classifier**: as n -> infinity, our model's accuracy approaches the bayes optimal limit

- Once you know enough about your distribution, you can perform really well but you have to have enough samples

- Another concept: how to reduce the bias
## Uncertainty forest
- learn the boxes with portion of training data: this portion is used to learn the structure
- put a portion of the training data that you did not use aside: this portion is pushed through the tree
- The posteriors do not depend on each other
- Finite posterior correction: a box may not be populated when i push a portion of my training data through (portion that my model was not trained on), we then assume that box is equally likely to be populated by each class

## BAGGING READ BRIEMAN 2001 - [here](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf), learn the basics, dont focus so hard on the math
- train multiple trees on different subsets of training data (proglearn uses 67% data of each tree by sampling with replacement)
- get posteriors using the out-of-bag data (33%) of each tree
500, 1000 training samples
1 tree -> 67% of 1000 & 33% hold out data (OOB Data)
there's an optimal amount of tree depending on the sample size

- depth controls overfitting

- prio decision forest book, then the textbook

- do gaussian sim's in proglearn tutorial, build intuition on simulation data first
