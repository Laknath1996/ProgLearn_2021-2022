- if your partitions are so small they become a point, you can perfectly learn the distribution, but you must have equal samples in each partition 

Stones theorem: if you have infinite samples you can reach bayes optimal, if you have inf data points you dont need to learn the partitions  

Today Jayanta will go over:
1. what is effect of partitions on each other?
2. How can we improve the partitions learned on bootstrap data?

- new partitions are not always helpful if you have an uniformative partition 
- eg; (0.8+0.5)/2
- a bad partition is called an adiversarial partition 
- limited data need to very careful with your partitions
- the goal is to see how well you can do with a limited amount of samples

- **oblique split:** split that is not axis aligned dont go through this [paper](https://arxiv.org/pdf/1507.05444.pdf) first
- Go through SPORF first: consider the k*45 degree split, where k = 0, 1, 2, 3
- Read Brieman next week if you can 

- Look over MORF: combine feature at each decision node as a patch
- CNN

git pull git checkout staging then again git pull 
pip uninstall proglearn

standardize using jupyter black 
