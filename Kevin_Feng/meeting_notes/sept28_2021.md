# Notes from 9/28/21, Jayanta presentation 
## What is a learning task?

![image](https://user-images.githubusercontent.com/89429238/135137797-e5409634-e786-416f-b229-9d6f72ef39e7.png)

- if any of these componenets change, then the task changes
- A shift in the task is a change in any one of these components

## What is unsupervised learning?
- when you are not given class labels

## What is forward learning?

## What is backward learning?

## What happens when you multiply forward and backward learning?
- you get transfer learning

## Simple example where we can do lifelong learning
![image](https://user-images.githubusercontent.com/89429238/135138623-fcd49218-e447-4260-85cb-531030ab3f96.png)

- model gn trained on data Dn, no reg = not constraining params to be something 
- Minimizes R, risk
- Generalization error is the expected risk
- train and test set should be independent of each other
- fixed cap = when you get more data you arent changing the size of the model
- new task data comes in so we need to retrain model gn

What do we need to consider now?
- similarity between the two tasks 
- gen error on the prev and current task

What will happen with your model when you feed it data with slightly shifted data (so they have slight over lap)?
- it will perform slightly better on the new task, gen error improves

How can we mitigate forgetting?
- use capacity efficiently, regularize the params (EWC, SI) <- this paper explains formula 

![image](https://user-images.githubusercontent.com/89429238/135140293-fb44be15-4233-444b-9513-d02e640552b2.png)

This method somewhat mitigates forgetting but not that much

- replay exact or similar represenations (inexact) of t he old task (LwF) (learning without forgetting algo)

![image](https://user-images.githubusercontent.com/89429238/135140493-d99643ad-011d-43ed-a13e-37cad9b3ccec.png)

- after intro replay, this is what our loss function looks like ^
- this is kinda like when we dream, revising what we have done 
- but if you have fixed cap, you will eventually forget after enough tasks

Maybe we can find a way to summarize what we learned before so it takes up less space?

What else can we do to mitigate forgetting?
- concat al lthe task data and train a model with increase cap per task
