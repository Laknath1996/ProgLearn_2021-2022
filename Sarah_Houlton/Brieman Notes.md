[Notes on the Brieman paper](http://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf)

# Random Forests
## Introduction
* random split selection
  * at each node the split is selected from k best splits
* random subspace
  * random selection of a subset of features to use to grow each tree
* Amit and Geman define a large number of geometric features and search over a random selection of these for the best split at each node
* common element
  * for kth tree, a random vector is generated, independent of other random vectors, but with the same distribution
  * tree is grown using training set and random vector
  * resulting in a classifier where x is an input vector   

## Characterizing the accuracy of random forests
* margin measures the extent to which the average number of votes at X, Y for the right class exceeds the average vote for any other class
  * larger margin, more confidence
* upper bound can be derived for generalization error in terms of two parameters that are measures of how accurate the individual classifiers are and the dependence between them
* The c/s2 ratio is the correlation divided by the square of the strength
  * keep small
 
## Using random features
* To improve accuracy, the randomness injected has to minimize the correlation ρ¯ while maintaining strength
  * Its accuracy is as good as Adaboost and sometimes better.
  * It’s relatively robust to outliers and noise.
  *  It’s faster than bagging or boosting.
  * It gives useful internal estimates of error, strength, correlation and variable importance.
  *  It’s simple and easily parallelized. 
* bagging used in tandem w/ random feature selection
  * each new training set drawn with replacement from original training set
* in each bootstrap training set ~1/3 of instances are left out
* out of bag estimates are based on combining about 1/3 of as many classifiers as in main combination
* error rate decreases as the number of combinations increases

## Random forests using random input selection
* simplest random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on
* grow the tree w/ CART methodology and do not prune
* small datasets
  * set aside 10% test data
  * random forest was run twice w/ different f's
  * test set was used to get error for both
  * same procedure run w/ adaboost
* larger datasets
  * zipcode used 200 trees
  * 50 for first 3 in adaboost, 100 for zipcode
* error rates for random selection compare favorably w/ adaboost
* random input selection can be faster than adaboost or bagging

## Random forests using linear combinations of inputs
* defining more features by taking random linear combinations of a number of the input variables
* feature is generated by specifying L, number of variables to be combined
  * at a given node, L variables are selecteed randomly and added together with uniform random coefficients btwn 0 and 1
* compares even more favorably than RI
* on some large datasets, increasing F led to improved performance
* each time a categorical variable is selected to split, select a random subset of categories of the variable and define a sub variable that is 1 when the value of the variable is in the subset and 0 outside

## Empirical results on strength and correlation
* In small datasets, the strength remains constant after a certain number of inputs
* test set errors show a small drop to begin, then a general, gradual increase
* with larger datasets, the strength continues to increase longer the stil plateaus
* better random forests have lower correlation between classifiers and higher strength

## Conjecture: Adaboost is a random forest
* Adaboost is a deterministic algorithm that selects the weights on the training set for input to the next classifier based on the misclassifications in the previous classifiers
* Adaboost can be equivalent to a random forest where the weights on the training set are selected at random from the distribution Qpi
* This wouldl explain why adaboost does not overfit as more trees are added to the ensemble

## The effects of output noise
* to test robustness with respect to noise, randomly altering 1 in 20 labels
* adaboost deteriorates markedly with the noise, random forest has small changes
* in adaboosst, incorrect labeling gets magnified, not so in random forest

## Data with many weak inputs
* data sets with many weak inputs are common
  * medical diagnosis
  * document retrieval
* with many weak inputs test data
  * bayes classifier has a 6.2% error rate
  * forest-RI with a high strength and low correlation - 2.8%
  * forest-RI have the ability to work with very weak classifiers as long as correlation is low
  
## Exploring the random forest mechanism
* dependent variables affect prediciton error
  * if both are noised separately, they will give the same increase in error rate
  * once one is a predictive variable, using the other will not result in a decreased error rate
  
## Empirical results in regression
* correlation increases slowly as the number of features increases
* a relatively large number of features are required to reduce PG(tree) and get new optimal testset error
* various combinations of randomness can be added to see what works best

## Remarks and conclusions
* random forests are an effective tool in prediction
  * especially with the right kind of randomness
* forests can compete with arcing type algorithms
* different injections of randomness produce different results 
