[Notes on the Brieman paper](http://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf)

# Random Forests
## Introduction
* random split selection
  * at each node the split is selected from k best splits
* random subspace
  * random selection of a subset of features to use to grow each tree
* Amit and Geman define a large number of geometric features and search over a random selection of these for the best split at each node
* common element
  * for kth tree, a random vector is generated, independent of other random vectors, but with the same distribution
  * tree is grown using training set and random vector
  * resulting in a classifier where x is an input vector   

## Characterizing the accuracy of random forests
* margin measures the extent to which the average number of votes at X, Y for the right class exceeds the average vote for any other class
  * larger margin, more confidence
* upper bound can be derived for generalization error in terms of two parameters that are measures of how accurate the individual classifiers are and the dependence between them
* The c/s2 ratio is the correlation divided by the square of the strength
  * keep small
 
## Using random features
* To improve accuracy, the randomness injected has to minimize the correlation ρ¯ while maintaining strength
  * Its accuracy is as good as Adaboost and sometimes better.
  * It’s relatively robust to outliers and noise.
  *  It’s faster than bagging or boosting.
  * It gives useful internal estimates of error, strength, correlation and variable importance.
  *  It’s simple and easily parallelized. 
* bagging used in tandem w/ random feature selection
  * each new training set drawn with replacement from original training set
* in each bootstrap training set ~1/3 of instances are left out
* out of bag estimates are based on combining about 1/3 of as many classifiers as in main combination
* error rate decreases as the number of combinations increases

## Random forests using random input selection
* simplest random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on
* grow the tree w/ CART methodology and do not prune
* small datasets
  * set aside 10% test data
  * random forest was run twice w/ different f's
  * test set was used to get error for both
  * same procedure run w/ adaboost
* larger datasets
  * zipcode used 200 trees
  * 50 for first 3 in adaboost, 100 for zipcode
* error rates for random selection compare favorably w/ adaboost
* random input selection can be faster than adaboost or bagging

## Random forests using linear combinations of inputs
* defining more features by taking random linear combinations of a number of the input variables
* feature is generated by specifying L, number of variables to be combined
  * at a given node, L variables are selecteed randomly and added together with uniform random coefficients btwn 0 and 1
* compares even more favorably than RI
* on some large datasets, increasing F led to improved performance
* each time a categorical variable is selected to split, select a random subset of categories of the variable and define a sub variable that is 1 when the value of the variable is in the subset and 0 outside

## Empirical results on strength and correlation
* 
