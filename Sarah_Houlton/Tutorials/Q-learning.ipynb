{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99270eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from collections         import deque\n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a148a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.weight_backup      = \"cartpole_weight.h5\"\n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.brain              = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        return model\n",
    "\n",
    "    def save_model(self):\n",
    "            self.brain.save(self.weight_backup)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.brain.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        for state, action, reward, next_state, done in sample_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
    "            target_f = self.brain.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1c2a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 100\n",
    "        self.env               = gym.make('CartPole-v1')\n",
    "\n",
    "        self.state_size        = self.env.observation_space.shape[0]\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.agent             = Agent(self.state_size, self.action_size)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            for index_episode in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "                done = False\n",
    "                index = 0\n",
    "                while not done:\n",
    "#                    self.env.render()\n",
    "\n",
    "                    action = self.agent.act(state)\n",
    "\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                    index += 1\n",
    "                print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\n",
    "                self.agent.replay(self.sample_batch_size)\n",
    "        finally:\n",
    "            self.agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b0ed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0# Score: 332\n",
      "Episode 1# Score: 120\n",
      "Episode 2# Score: 501\n",
      "Episode 3# Score: 190\n",
      "Episode 4# Score: 128\n",
      "Episode 5# Score: 134\n",
      "Episode 6# Score: 186\n",
      "Episode 7# Score: 132\n",
      "Episode 8# Score: 105\n",
      "Episode 9# Score: 159\n",
      "Episode 10# Score: 121\n",
      "Episode 11# Score: 147\n",
      "Episode 12# Score: 110\n",
      "Episode 13# Score: 118\n",
      "Episode 14# Score: 114\n",
      "Episode 15# Score: 121\n",
      "Episode 16# Score: 143\n",
      "Episode 17# Score: 111\n",
      "Episode 18# Score: 114\n",
      "Episode 19# Score: 87\n",
      "Episode 20# Score: 116\n",
      "Episode 21# Score: 68\n",
      "Episode 22# Score: 111\n",
      "Episode 23# Score: 103\n",
      "Episode 24# Score: 132\n",
      "Episode 25# Score: 99\n",
      "Episode 26# Score: 115\n",
      "Episode 27# Score: 130\n",
      "Episode 28# Score: 122\n",
      "Episode 29# Score: 101\n",
      "Episode 30# Score: 101\n",
      "Episode 31# Score: 86\n",
      "Episode 32# Score: 116\n",
      "Episode 33# Score: 114\n",
      "Episode 34# Score: 107\n",
      "Episode 35# Score: 78\n",
      "Episode 36# Score: 91\n",
      "Episode 37# Score: 98\n",
      "Episode 38# Score: 122\n",
      "Episode 39# Score: 118\n",
      "Episode 40# Score: 110\n",
      "Episode 41# Score: 113\n",
      "Episode 42# Score: 82\n",
      "Episode 43# Score: 135\n",
      "Episode 44# Score: 471\n",
      "Episode 45# Score: 235\n",
      "Episode 46# Score: 64\n",
      "Episode 47# Score: 445\n",
      "Episode 48# Score: 501\n",
      "Episode 49# Score: 371\n",
      "Episode 50# Score: 270\n",
      "Episode 51# Score: 140\n",
      "Episode 52# Score: 501\n",
      "Episode 53# Score: 307\n",
      "Episode 54# Score: 501\n",
      "Episode 55# Score: 59\n",
      "Episode 56# Score: 52\n",
      "Episode 57# Score: 100\n",
      "Episode 58# Score: 30\n",
      "Episode 59# Score: 31\n",
      "Episode 60# Score: 319\n",
      "Episode 61# Score: 104\n",
      "Episode 62# Score: 88\n",
      "Episode 63# Score: 16\n",
      "Episode 64# Score: 46\n",
      "Episode 65# Score: 154\n",
      "Episode 66# Score: 501\n",
      "Episode 67# Score: 379\n",
      "Episode 68# Score: 439\n",
      "Episode 69# Score: 78\n",
      "Episode 70# Score: 144\n",
      "Episode 71# Score: 124\n",
      "Episode 72# Score: 211\n",
      "Episode 73# Score: 132\n",
      "Episode 74# Score: 501\n",
      "Episode 75# Score: 405\n",
      "Episode 76# Score: 166\n",
      "Episode 77# Score: 165\n",
      "Episode 78# Score: 165\n",
      "Episode 79# Score: 85\n",
      "Episode 80# Score: 382\n",
      "Episode 81# Score: 458\n",
      "Episode 82# Score: 398\n",
      "Episode 83# Score: 322\n",
      "Episode 84# Score: 356\n",
      "Episode 85# Score: 37\n",
      "Episode 86# Score: 501\n",
      "Episode 87# Score: 498\n",
      "Episode 88# Score: 58\n",
      "Episode 89# Score: 264\n",
      "Episode 90# Score: 291\n",
      "Episode 91# Score: 295\n",
      "Episode 92# Score: 62\n",
      "Episode 93# Score: 68\n",
      "Episode 94# Score: 71\n",
      "Episode 95# Score: 266\n",
      "Episode 96# Score: 501\n",
      "Episode 97# Score: 162\n",
      "Episode 98# Score: 37\n",
      "Episode 99# Score: 197\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cartpole = CartPole()\n",
    "    cartpole.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84134b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlearning",
   "language": "python",
   "name": "qlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
