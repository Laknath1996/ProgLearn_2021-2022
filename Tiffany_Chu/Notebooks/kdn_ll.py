# -*- coding: utf-8 -*-
"""kdn_ll.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AJOf_L8TJwfMq5hxgRge1MIoLxc6H083
"""

#install libraries

!rm -r kdg
!git clone -b weighted_kdn https://github.com/NeuroDataDesign/kdg
!ls
!pip install kdg/.

# import modules
import numpy as np
from tensorflow import keras
from keras import layers
from kdg.utils import gaussian_sparse_parity, trunk_sim
import pandas as pd
import os

import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive 
drive.mount('/content/drive')
os.chdir("/content/drive/My Drive/NDD/Weighted_KDN")
!pwd

from kdg import KernelDensityGraph
from sklearn.mixture import GaussianMixture
from tensorflow import keras
from sklearn.utils.validation import check_array, check_is_fitted, check_X_y
import numpy as np
from scipy.stats import multivariate_normal
import warnings
from sklearn.covariance import LedoitWolf
import matplotlib.pyplot as plt

class kdn_test(KernelDensityGraph):

    def __init__(self,
        network,
        covariance_types = 'full', 
        criterion=None,
        mode="LL",
        penultimate=False):
        super().__init__()
        self.polytope_means = {}
        self.polytope_cov = {}
        self.network = network
        self.covariance_types = covariance_types
        self.criterion = criterion
        self.mode = mode
        self.penultimate = penultimate
        #Does this hold for CNN?
        self.num_fc_neurons = [l.output.shape[1] for l in network.layers]

    def _get_polytope_memberships(self, X):
        polytope_memberships = []
        last_activations = X
        total_layers = len(self.network.layers)

        # Iterate through neural network manually, getting node activations at each step
        for layer_id in range(total_layers):
            weights, bias = self.network.layers[layer_id].get_weights()

            # Calculate new activations based on input to this layer
            preactivation = np.matmul(last_activations, weights) + bias

             # get list of activated nodes in this layer
            if layer_id == total_layers - 1:
                binary_preactivation = (preactivation > 0.5).astype('int')
            else:
                binary_preactivation = (preactivation > 0).astype('int')
            
            # convert to binary to int
            polytope_id = np.tensordot(binary_preactivation, 2 ** np.arange(0, bias.shape[0]), axes = 1)
            if self.penultimate:
              # determine the polytope memberships only based on the penultimate layer
              if layer_id == total_layers - 2:
                polytope_memberships.append(polytope_id)
                
            else: 
              # determine the polytope memberships based on all the FC layers
              polytope_memberships.append(polytope_id)
            
            #print(binary_preactivation.shape)
            #print(polytope_id.shape)
            # remove all nodes that were not activated
            last_activations = preactivation * binary_preactivation

        #Concatenate all activations for given observation
        #polytope_obs = np.concatenate(polytope_memberships, axis = 1)
        #polytope_memberships = [np.tensordot(polytope_obs, 2 ** np.arange(0, np.shape(polytope_obs)[1]), axes = 1)]

        polytope_memberships = np.asarray(polytope_memberships).T

        print(polytope_memberships.shape)

        return polytope_memberships
                           

    def fit(self, X, y):
        r"""
        Fits the kernel density forest.
        Parameters
        ----------
        X : ndarray
            Input data matrix.
        y : ndarray
            Output (i.e. response) data matrix.
        """
        X, y = check_X_y(X, y)
        self.labels = np.unique(y)

        feature_dim = X.shape[1]
        
        for label in self.labels:
            self.polytope_means[label] = []
            self.polytope_cov[label] = []

            X_ = X[np.where(y==label)[0]]
            polytope_memberships = self._get_polytope_memberships(X_)
            unique_polytopes = np.unique(polytope_memberships, axis=0) # get the unique polytopes
            print("Number of Polytopes : ", len(polytope_memberships))
            print("Number of Unique Polytopes : ", len(unique_polytopes))
            
            polytope_member_count = [] # store the polytope member counts
            for polytope in unique_polytopes: # fit Gaussians for each unique non-singleton polytope
                #count matches
                n_layers_match = polytope_memberships == polytope
                total_match = sum(np.all(n_layers_match, axis=1))
                polytope_member_count.append(total_match)

                if total_match < 3: # don't fit a gaussian to polytopes that has less perfect members than the specified threshold
                    continue

                # get the activation pattern of the current polytope
                #current_polytope_activation = np.binary_repr(polytope, width=self.num_fc_neurons)[::-1] 

                # compute the weights
                weights = []
                for member in polytope_memberships:
                    n_mismatches = []
                    #iterate through layers of this member's polytope
                    for i, mi in enumerate(member):
                        layer_match = mi == polytope[i]
                        if layer_match: n_mismatches.append(0)
                        else: 
                            # Calculate binary XOR between these numbers, count set bits
                            hamming_dist = sum([i=="1" for i in np.binary_repr(mi ^ polytope[i])])
                            n_mismatches.append(hamming_dist)
                            if self.mode == "FM" :
                                #count every other layer as 100% mismatched from here
                                n_mismatches.extend(self.num_fc_neurons[i+1:])
                                break
                    #print(n_mismatches)
                    if self.mode == "TM" or self.mode == "FM":
                        #scale by number of mismatched nodes
                        weight = 1 - sum(n_mismatches)/sum(self.num_fc_neurons)
                    elif self.mode == "LL":
                        #scale every layer equally
                        weight = 1 - np.sum(np.array(n_mismatches)/np.array(self.num_fc_neurons))/len(member)
                    weights.append(weight)
                weights = np.array(weights)
                #print(weights)

                X_tmp = X_.copy()
                polytope_mean_ = np.average(X_tmp, axis=0, weights=weights) # compute the weighted average of the samples 
                X_tmp -= polytope_mean_ # center the data

                sqrt_weights = np.sqrt(weights)
                sqrt_weights = np.expand_dims(sqrt_weights, axis=-1)
                X_tmp *= sqrt_weights # scale the centered data with the square root of the weights

                # compute the paramters of the Gaussian underlying the polytope
                 
                # # Gaussian Mixture Model (uncomment)
                # gm = GaussianMixture(n_components=1, covariance_type=self.covariance_types, reg_covar=1e-4).fit(X_[idx])
                # polytope_mean_ = gm.means_[0]
                # polytope_cov_ = gm.covariances_[0]
                
                # LedoitWolf Estimator (uncomment)
                covariance_model = LedoitWolf(assume_centered=True)
                covariance_model.fit(X_tmp)
                polytope_cov_ = covariance_model.covariance_ * len(weights) / sum(weights)

                # store the mean and covariances
                self.polytope_means[label].append(
                        polytope_mean_
                )
                self.polytope_cov[label].append(
                        polytope_cov_
                )
            plt.hist(polytope_member_count, bins=30)
            plt.xlabel("Number of Members")
            plt.ylabel("Number of Polytopes")
            plt.show()

    def _compute_pdf(self, X, label, polytope_idx):
        polytope_mean = self.polytope_means[label][polytope_idx]
        polytope_cov = self.polytope_cov[label][polytope_idx]

        var = multivariate_normal(
            mean=polytope_mean, 
            cov=polytope_cov, 
            allow_singular=True
            )

        likelihood = var.pdf(X)
        return likelihood

    def predict_proba(self, X):
        r"""
        Calculate posteriors using the kernel density forest.
        Parameters
        ----------
        X : ndarray
            Input data matrix.
        """
        X = check_array(X)

        likelihoods = np.zeros(
            (np.size(X,0), len(self.labels)),
            dtype=float
        )
        
        for ii,label in enumerate(self.labels):
            for polytope_idx,_ in enumerate(self.polytope_means[label]):
                likelihoods[:,ii] += np.nan_to_num(self._compute_pdf(X, label, polytope_idx))

        proba = (likelihoods.T/(np.sum(likelihoods,axis=1)+1e-100)).T
        return proba

    def predict(self, X):
        r"""
        Perform inference using the kernel density forest.
        Parameters
        ----------
        X : ndarray
            Input data matrix.
        """
        return np.argmax(self.predict_proba(X), axis = 1)

#experiment layout

sample_size = [1000, 5000, 10000] # sample size under consideration
n_test = 1000 # test set size
reps = 10 # number of replicates

experiment = "2021_10_21_"

p = 20 # total dimensions of the data vector
p_star = 3 # number of signal dimensions of the data vector

compile_kwargs = {
    "loss": "binary_crossentropy",
    "optimizer": keras.optimizers.Adam(3e-4)
    }
fit_kwargs = {
    "epochs": 150,
    "batch_size": 32,
    "verbose": False
    }

def getNN():
    network_base = keras.Sequential()
    network_base.add(layers.Dense(5, activation='relu', input_shape=(20,)))
    network_base.add(layers.Dense(5, activation='relu'))
    network_base.add(layers.Dense(units=2, activation = 'softmax'))
    network_base.compile(**compile_kwargs)
    return network_base

for mode in ["LL", "FM", "TM"]:
    df = pd.DataFrame()
    reps_list = []
    accuracy_kdn = []
    accuracy_kdn_ = []
    accuracy_nn = []
    accuracy_nn_ = []
    sample_list = []
    print(f"Using weighting mode {mode}")
    for sample in sample_size:
        print('Doing sample %d'%sample)
        for ii in range(reps):
            X, y = gaussian_sparse_parity(
                sample,
                p_star=p_star,
                p=p
            )
            X_test, y_test = gaussian_sparse_parity(
                n_test,
                p_star=p_star,
                p=p
            )

            # train Vanilla NN
            vanilla_nn = getNN()
            vanilla_nn.fit(X, keras.utils.to_categorical(y), **fit_kwargs)

            # train KDN
            model_kdn = kdn_test(network=vanilla_nn, mode=mode, penultimate=False)
            model_kdn.fit(X, y)

            accuracy_kdn.append(
                np.mean(
                    model_kdn.predict(X_test) == y_test
                )
            )
            
            accuracy_nn.append(
                np.mean(
                    np.argmax(vanilla_nn.predict(X_test), axis=1) == y_test
                )
            )
            reps_list.append(ii)
            sample_list.append(sample)
        print("NN Accuracy:", accuracy_nn)
        print("KDN Accuracy:", accuracy_kdn)
    df['accuracy kdn'] = accuracy_kdn
    df['accuracy nn'] = accuracy_nn
    df['reps'] = reps_list
    df['sample'] = sample_list

    # save the results (CHANGE HERE)
    df.to_csv(f'{experiment}high_dim_kdn_gaussian_weighting_{mode}.csv')