Breiman L. Machine Learning. 2001;45(1):5-32. doi:10.1023/a:1010933404324 via https://link.springer.com/article/10.1023/A:1010933404324

## Section 1: Introduction

**Definition 1.1.** A random forest is a classifier consisting of a collection of tree-structured classifiers {*h(x, &Theta;<sub>k</sub>)*, *k* = 1,2, etc....} where: 
 - *x* is an input, i.e. something that that needs to be labeled with a given *class*
 - *&Theta;<sub>k</sub>* is a random vector which controls the nature of tree growth, e.g.:
    - a subset of the training data available, used to train the classifier
    - a list of random directions to take at each split
    - random weights applied to the entire training set
 - *h(x, &Theta;<sub>k</sub>)* is the classifier which selects the class for *x* based on *&Theta;<sub>k</sub>* 

The *random forest* polls all the trees and selects the most popular class for *x* based on the trees' votes.

## Section 2: Characterizing the accuracy of random forests

The *margin* measures the extent to which the average number of votes at X, Y for the right class exceeds the average vote for any other class.

> margin = margin(X, Y) = avg(as.binary(h<sub>k</sub>(X)==Y)) - max([avg(as.binary(h<sub>k</sub>(X) == j)) for j != y])
> 
> (see: Tiffany_Chu/PaperReviews/20210907_NewMLAlgorithmRF.md)

Per Breiman, *h<sub>k</sub>(X)* = *h(x, &Theta;<sub>k</sub>)*

as.binary(f(x)) can be written as *I(x)*. I is the Indicator function - 1 if TRUE, 0 if FALSE, i.e. boolean logic

Note: margin < 0 if some erroneous label *j* is more popular than the correct classifications

#### Generalization Error

The generalization error *PE\* = P<sub>X,Y</sub>(margin(X,Y) < 0)*

i.e. what is the probability that the margin will be negative, i.e. error is more common than correct label

Via Strong Law of Large Numbers: *PE\** will converge to a negative number

#### Strong Law of Large Numbers (Kolmogorov's law):
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ef24c64d62449cf8598043aa83f9092028924bec">

> via wikipedia https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law

i.e. the average of a sample will converge to the expected value when given enough samples.

#### Strength of a forest

The strength of a forest {*h(x, &Theta;<sub>k</sub>)*, *k* = 1,2, etc....} is denoted *s* and is equal to expected value (E) * margin. We are more *confident* in a stronger forest.

## Section 3: Using random features

Breiman says Adaboost [adaptive boosting] is the best (lowest generalization error) random forest algorithm in 2001, beating:
- bagging / B[ootstrap] AG[gregation] (worst) [select training data subset from all available training data, with replacement]
- random split selection (choose division points for branch nodes at random)
- adding noise to outputs
- adaptive reweighting of training data (change how much consideration you give to each piece of training data) 

> Question: What is boosting?

9/22/2021: Adaboost is a generalized algorithm that ensembles many weak classifiers and trains them by exponentially reweighting the inputs by *e<sup>&alpha;t</sup>* where *&alpha;* is the learning rate and *t* is the number of iterations. Correct answers are reduced in weight and incorrect answers are increased in weight, such that Adaboost "focuses" more on getting these answers right next time. The weak classifiers are finally polled to get the correct/ensemble classifications.

Methods can be combined - Breiman combines bagging with random feature selection to grow his trees.

The benefit of bagging: can use *out of bag* (y, x not included in training subset) data to estimate error. This removes the need for separate training/testing data.

## Section 4: Random Features/Forest-RI

Breiman tries growing trees on several datasets, splitting on F = 1 *features* or F = int(log<sub>2</sub>M+1), M = nFeatures (or inputs, as he calls them - think columns in a table).

For small datasets (nData < 2500) 10% of data is OOB
For large/synthetic datasets, training and testing sets were set aside beforehand
2x as many runs for zip code data, which has >4x as many features as next most feature-heavy dataset (256 vs 60)
- RF is run 100 times with F=1 and F=int(log<sub>2</sub>M+1)
- Adaboost is run 50 times with F=1 and F=int(log<sub>2</sub>M+1)
- OOB data is run through forest to get *test error* (only covers about 1/3 of the trees)

RF is significantly faster: "Growing the 100 trees in random forests was considerably quicker than the 50 trees for Adaboost." (Breiman 2001)

Random feature/input selection compares favorably with Adaboost, and is faster. This holds true even when trees were grown on single variable - does this imply that many of these variables are correlated?

Forest-RI with F=25 performed best on zipcode data, based on "hunch"

## Section 5: Linear Combinations/Forest-RC

Combined features are generated from linear combinations of L features/inputs.

Decided pre-hoc that L = 3 s.t. a feature RC = ax<sub>1</sub>+bx<sub>2</sub>+cx<sub>3</sub>, x<sub>1/2/3</sub> are inputs

For small datasets, F=2 linear combinations were generated at each node and the best one was chosen
For large datasets, F=8 linear combinations were generated at each node and the best one was chosen

Forest-RC does best on synthetic data, but still performs comparably to Adaboost on real data, and performance increases on large, but not small, datasets up to F=200

For categorical variables (ex: ethnicity), Breiman creates indicator function I, s.t. I = 1 when x is in one of a random subset {x<sub>1</sub>...x<sub>k</sub>} categories, and 0 otherwise

Since a categorical variable with *I* values can be coded into *I-1* dummy boolean variables, he increases the probability of selecting the categorical variable by *I-1*x, to make sure we choose it enough times

*F* must be increased to about (2-3)x int(log<sub>2</sub>M+1) to get enough strength to provide good test set accuracy especially on Forest-RI.
For 3+ classes, the search for the best categorical split is an O(2<sup>I −1</sup>) computation

## Section 6: Sonar & Satellite Data

This section is a more in-depth look at Forest-RI and Forest-RC as applied to the sonar and breast (small) and satellite (large) datasets.

Breiman concludes that better forests have lower correlation between classifiers (trees) but maintain high strength. This makes sense - the forest is more diverse if the trees are not correlated.

## Section 7: Adaboost as a random forest

Conjecture: Adaboost, though the algorithm itself is deterministic, can be portrayed as a random forest with the weights standing in for the randomization - i.e. multiplying the terms by the adaptive weight effectively creates a random forest with the weights chosen randomly from a distribution *Qπ* where *Q* is the weight and *π* is the invariant measure (look up *ergodic*)

## Section 8: Adaboost and noise

Adaboost's accuracy plummets when noise is added post-processing to the inputs (ex: by randomly re-labeling 5% of the inputs after each pass), which makes sense as Adaboost deterministically prioritizes certain datapoints depending on how far they are from their correct label. RI and RC are more resilient.

## Section 9: Weak Data

For broad/weak data (many poorly correlated inputs/features), Forest-RI performs similarly to a naive Bayesian classifier (assuming a probability is simply made up of a number of linear relations), while Adaboost simply doesn't run. Particularly relevant for large-scale biological data, ex: medical records, WGS.

## Section 10: Understanding Random Forests

While it's hard to understand how Random Forests work, we can perturb the forest by running the OOB data with a select feature *m* noised up i.e. randomly permuted across all OOB data. Run this for each variable *1, 2, ....M* and compare to the true labels to see which noisy feature most adversely affects labeling.

Likewise, run Forest-RI with splits on *F=m* to determine how accurately the single variable can sort the data. 

Sometimes, two variables can appear individually impactful but adding one to the other doesn't create any increase in accuracy - this indicates that these two variables are dependent.

## Sections 11 and 12: Regression with Forests

By letting our trees take on vector values instead of class values, a forest can perform regression. An ideal regression forest should have low correlation between residuals (between our data and the tree's line), and low error (so the tree should not be bad at regression). RF-RI beats bagging, and a large number of features is required. 

## Section 13: Conclusions

The Law of Large Numbers means that random forests do not overfit, and with appropriate randomness (ex: boosting, bagging) they can be extremely accurate classifiers. OOB data provides a valuable way to concretely verify the strength of trees and the value of each feature. A benefit of forests is that they do not perturb the original training data, only resample it.
