Breiman L. Machine Learning. 2001;45(1):5-32. doi:10.1023/a:1010933404324 via https://link.springer.com/article/10.1023/A:1010933404324

## Section 1: Introduction

**Definition 1.1.** A random forest is a classifier consisting of a collection of tree-structured classifiers {*h(x, &Theta;<sub>k</sub>)*, *k* = 1,2, etc....} where: 
 - *x* is an input, i.e. something that that needs to be labeled with a given *class*
 - *&Theta;<sub>k</sub>* is a random vector which controls the nature of tree growth, e.g.:
    - a subset of the training data available, used to train the classifier
    - a list of random directions to take at each split
    - random weights applied to the entire training set
 - *h(x, &Theta;<sub>k</sub>)* is the classifier which selects the class for *x* based on *&Theta;<sub>k</sub>* 

The *random forest* polls all the trees and selects the most popular class for *x* based on the trees' votes.

## Section 2: Characterizing the accuracy of random forests

The *margin* measures the extent to which the average number of votes at X, Y for the right class exceeds the average vote for any other class.

> margin = margin(X, Y) = avg(as.binary(h<sub>k</sub>(X)==Y)) - max([avg(as.binary(h<sub>k</sub>(X) == j)) for j != y])
> 
> (see: Tiffany_Chu/PaperReviews/20210907_NewMLAlgorithmRF.md)

Per Breiman, *h<sub>k</sub>(X)* = *h(x, &Theta;<sub>k</sub>)*

as.binary(f(x)) can be written as *I(x)*. I is the Indicator function - 1 if TRUE, 0 if FALSE, i.e. boolean logic

Note: margin < 0 if some erroneous label *j* is more popular than the correct classifications

#### Generalization Error

The generalization error *PE\* = P<sub>X,Y</sub>(margin(X,Y) < 0)*

i.e. what is the probability that the margin will be negative, i.e. error is more common than correct label

Via Strong Law of Large Numbers: *PE\** will converge to a negative number

#### Strong Law of Large Numbers (Kolmogorov's law):
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ef24c64d62449cf8598043aa83f9092028924bec">

> via wikipedia https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law

i.e. the average of a sample will converge to the expected value when given enough samples.

#### Strength of a forest

The strength of a forest {*h(x, &Theta;<sub>k</sub>)*, *k* = 1,2, etc....} is denoted *s* and is equal to expected value (E) * margin

## Section 3: Using random features

Breiman says Adaboost [adaptive boosting] is the best (lowest generalization error) random forest algorithm in 2001, beating:
- bagging / B[ootstrap] AG[gregation] (worst) [select training data subset from all available training data, with replacement]
- random split selection (choose division points for branch nodes at random)
- adding noise to outputs
- adaptive reweighting of training data (change how much consideration you give to each piece of training data) 

> Question: What is boosting?

Methods can be combined - Breiman combines bagging with random feature selection to grow his trees.
