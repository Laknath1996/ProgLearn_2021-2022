{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Omnidirectional Transfer for Quasilinear Lifelong Learning\n",
    "\n",
    "[![Paper](https://img.shields.io/badge/Paper-arXiv-green)](https://arxiv.org/pdf/2004.12908.pdf)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* In biological learning, the learning is lifelong, with agents conitnually building on past knowledge and experiences, improving on many tasks given data associated with any task. (e.g. learning a second language improves the individual's performance in his native language)\n",
    "* Even though classical ML can simultaneously optimize for multiple tasks, if is difficult to sequentially optimize for multiple tasks.\n",
    "* Catastrophic forgetting: performance on the prior tasks drop precipitously upon training on new tasks.\n",
    "* Biological learning doesn't suffer from catastrophic forgetting\n",
    "* Two camps of overcoming catastrophic forgetting:\n",
    "    * Fixed resources, and therefore, reallocate resources (compressing representations) to incorporate new knowledge (**biologically, this is adulthood**)\n",
    "    * Adds resources to incorporate new knowledge (**biologically, this is (juvenile) development**)\n",
    "* The inability to omnidirectionally transfer is one of the key liming factors of AI.\n",
    "* In ProgNN, new tasks yield additional representational capacity. ProgNN can transfer forward, but they cannot transfer backward.\n",
    "\n",
    "## Main Contributions\n",
    "\n",
    "* Representational ensembling that enables omnidirectional transfer via an \"Omni-voter\" layer\n",
    "* Computational time reduced from quadratic to quasilinear\n",
    "* Two types of omnidirectional learning algorithms: \n",
    "    * Omnidirectional Forests (ODIF)\n",
    "    * Omnidirectional Networks (ODIN)\n",
    "* ODIN and ODIF are resource building (juvenile) but since they can leverage prior representations, they can convert in to an resouce recruiting (adult) state too. \n",
    "\n",
    "## Background\n",
    "\n",
    "### Classical ML\n",
    "\n",
    "* Consider RVs $(X, Y) \\sim P_{X,Y} where $X \\sim \\mathcal{X}$ is the input and $Y \\sim \\mathcal{Y}$ is the label. \n",
    "* $P_{X,Y} \\in \\mathcal{P}_{X,Y}$ is the joint distribution of $(X, Y)$\n",
    "* Let $l \\colon \\mathcal{Y} \\times \\mathcal{Y} \\longrightarrow (0, \\infty]$ be a loss function\n",
    "* The goal of classifcal ML is to find the hypothesis (predictor/ decision rule) $h \\colon \\mathcal{X} \\longrightarrow \\mathcal{Y}$ that minimizes te expected loss or *risk*,\n",
    "    $$ R(h) = \\mathbb{E}_{X, Y}[l(h(X), Y)] $$\n",
    "* A learning algorithm is a function $f$ that maps a dataset $\\bf{S}_n = \\{ X_i, Y_i \\}_{i=1}^n$. \n",
    "* If $n$ samples of $(X, Y)$ is i.i.d from some true but unknown $P_{X,Y}$, the generalization error or expected risk is given by,\n",
    "    $$ \\mathbb{E}[R(f(\\bf{S}_n))]$$\n",
    "* The goal: choose a learner $f$ that learns a hypothesis $h$ that has a small generalization error for the given task.\n",
    "\n",
    "### Lifelong Learning (LL)\n",
    "\n",
    "* Lifelong learning generalizes classifcal ML in the following ways:\n",
    "    * environment of $\\mathcal{T}$ tasks instead of a single task\n",
    "    * data arrive sequentially, instead of batch mode\n",
    "    * computational complexity contraints on the learning algoritm and hypotheses\n",
    "* Goal of LL: given new data and a new task, use all the exisiting data to achieve a lower generalization error on the new task, while also using the new data to obtain a lower generalization error on the previous tasks.\n",
    "* previous work: \n",
    "    * updating a fixed parametric model as new tasks arrive\n",
    "    * adding resources as new tasks arrive\n",
    "    * store/ replay previously encountered data to reduce forgetting\n",
    "* Task-aware: the learner is aware of all-task details for all tasks $h \\colon \\mathcal{X} \\times \\mathcal{T} \\longrightarrow \\mathcal{Y}$.\n",
    "* Task-unaware (task-agnostic): learner may not know that the task has changed at all $h \\colon \\mathcal{X} \\longrightarrow \\mathcal{Y}$.\n",
    "\n",
    "### Reference Algorithms\n",
    "\n",
    "**Resource Building Algorithms**: Progressive Neural Nets (ProgNN), Deconvolution-Factorized CNNs (DF-CNNs)\n",
    "\n",
    "**Fixed Capacity Algorithms**: Elastic Weight Consolidation (EWC), Online-EWC (O-EWC), Synaptic Intelligence (SI), Learning without Forgetting (LwF), ‘None’ and two variants of exact replay (Total Replay and Partial Replay).\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "===============================\n",
    "\n",
    "* The concept of omnidirectional transfer of knowledge is proposed to overcome the issue of catastrophic forgetting. \n",
    "\n",
    "* Through omnidirectional transfer, it is possible to realize the goal of lifelong learning, which is to improve the performance on a new task using knowledge about existing tasks and their data, while improving the performance on the previous tasks using the knowledge about new tasks and their data. \n",
    "\n",
    "* Introducing the concept of representational ensembling (through the Omni-voter) to accomplish the omnidirectional transfer is quite intuitive and its success is backed by experimental evidence mentioned in the manuscript.\n",
    "\n",
    "* This work further uses progressive learning concepts to incorporate resource building and resource recruitment into the proposed algorithms. \n",
    "\n",
    "# Potential Future directions? \n",
    "\n",
    "* How can we expand the proposed learning framework into task-agnostic situations? \n",
    "* How can we integrate tasks from different domains? (for instance, training an algorithm to learn two distinctive computer vision tasks. e.g. task 1: object recognition and task 2: semantic segmentation) \n",
    "* Would there be an upper bound / practical limitation for increasing representational capacity as the number of tasks fed into the network increases?\n",
    "* How can we extend the concept of omnidirectional transfer to learning algorithms other than PLNs and PLFs? (and for the situations where the hypotheses that cannot be readily decomposed into discernible transformers, voters, and deciders)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}