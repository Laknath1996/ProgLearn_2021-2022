{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction: The Abstract Forest Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction \n",
    "\n",
    "Prototypical ML tasks: \n",
    "* Classification (output is a label)\n",
    "* Regression (output is a continuous variable)\n",
    "* Density estimation (estimating a PDF)\n",
    "* Semi-supervised learning\n",
    "* Active learning (learning using minimum amount of ground-truths)\n",
    "\n",
    "Through a unified model of decision forests, we can map the prototypical ML problems onto the same general model by means of different paramterizations.\n",
    "\n",
    "Decision forests achieve generalization through ensembling slightly different trees.\n",
    "\n",
    "## Decision Tree Basics\n",
    "\n",
    "### Tree Data Structure\n",
    "\n",
    "* A tree is a graph that has a hierachical structure.\n",
    "* Nodes are either internal (split) or terminal (leaf) nodes.\n",
    "* Each node (except the root) have exactly one incoming edge.\n",
    "* A tree doesn't contain loops.\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "* Solving a complex problem by running a series of simpler tests\n",
    "* For a given input, the decision tree estimates an unknown property of the input by asking successive questions about its known properties\n",
    "* The next question depend on the answer to the previous question\n",
    "* The decision is made based on the terminal node\n",
    "* This mechanism can be graphically represented (path followed by the input through the questions)\n",
    "* All the questions asked move our decision making towards the correct region of the decision space\n",
    "* The more question asked, the more higher the confidence in the response\n",
    "* The tests are represented by a decision tree structure. \n",
    "    * Internal nodes: one question/ test each\n",
    "    * Root node: where the input is injected\n",
    "    * Leaf node: predictor (contains the most probable answer based on the questions asked during the tree descent)\n",
    "* Summary: **A decision tree is a tree where each internal node stores a split (or test) function to be applied to the incoming data. Each leaf stores the final answer (predictor). It is a hierachical piecewise model that splits complex problems into simpler ones.** \n",
    "\n",
    "### Mathematical Notations and Basic Defitions\n",
    "\n",
    "#### Data Points and Features\n",
    "\n",
    "* A data point is given by, \n",
    "    $$ \\mathbf{v} = (x_1, x_2, \\dots, x_d) \\in \\mathbb{R}^d $$\n",
    "where $x_i$ is a feature of the data point and $d$ is the feature space dimension (could be infinitely large, but we can pick a subset)\n",
    "* Features of interest are given by,\n",
    "    $$ \\mathbf{\\phi (v)} = (x_{\\phi_1}, x_{\\phi_2}, \\dots, x_{\\phi_{d'}}) \\in \\mathbb{R}^{d'} $$\n",
    "where $d' << d$ and $\\phi_i \\in [1, d]$.\n",
    "\n",
    "#### Test Functions, Split Functions, and Weak Learners\n",
    "\n",
    "* Each node has a different associated test function.\n",
    "* Test function (split function, weak learners) at a split node $j$ is given by, \n",
    "    $$ h(v, \\theta_j): \\mathbb{R}^d \\times \\mathcal{T} \\rightarrow \\{0, 1\\} $$\n",
    "where, $\\theta_j$ are the split parameters of the node $j$.\n",
    "\n",
    "#### Training Points and Training Sets\n",
    "\n",
    "* In a training data point, the attributes we are seeking may be known and could be be used to compute tree parameters.\n",
    "* Definitions: \n",
    "    * $\\mathcal{S}_0$: Training set\n",
    "    * $\\mathcal{S}_j$: Subset of training points reaching node $j$ (nodes are numbered breadth-first)\n",
    "    * $\\mathcal{S}_j^L$: Subset going to the left child of node $j$\n",
    "    * $\\mathcal{S}_j^R$: Subset going to the left child of node $j$\n",
    "* Dataset properties: \n",
    "    * $\\mathcal{S}_j = \\mathcal{S}_j^L \\cup \\mathcal{S}_j^R$\n",
    "    * $\\mathcal{S}_j^L \\cap \\mathcal{S}_j^R =  \\emptyset$\n",
    "    * $\\mathcal{S}_j^L = \\mathcal{S}_{2j+1}$\n",
    "    * $\\mathcal{S}_j^R = \\mathcal{S}_{2j+2}$\n",
    "\n",
    "#### Randomly Trained Decision Trees\n",
    "\n",
    "* **on-line phase: testing**\n",
    "    * The node tests have been selected (by training) and fixed during testing.\n",
    "    * Starting at the root, each split node applies the $h(\\cdot, \\cdot)$ to $\\mathbf{v}$. \n",
    "    * Depending on the result, $\\mathbf{v}$ is sent to the left or right child.\n",
    "    * This is repeated until a leaf node is reached. \n",
    "    * Leaf has a predictor/ estimator (e.g. classifier or a regressor) which assigns a lable to $\\mathbf{v}$.\n",
    "* **off-line phase: training**\n",
    "    * Split functions could be handcrafted / automatically learnt from data.\n",
    "    * Training phase select the type and params of the test function $h(v, \\theta)$ of each split node $j$ by optimizing a chose objective function on an available training set.\n",
    "    * This optimization happens in a greedy manner.\n",
    "    * At each node $j$, based on the incoming training set $\\mathcal{S}_j$, a function is learnt that best splits $\\mathcal{S}_j$ in to $\\mathcal{S}_j^L$ and $\\mathcal{S}_j^R$. This problem is written as, \n",
    "        $$ \\theta_j = \\argmax_{\\theta \\in \\mathcal{T}} I(\\mathcal{S}_j, \\theta) $$\n",
    "    where, $I$ is an objective function.\n",
    "    * This is done by a simple search over a discrete set of samples.\n",
    "    * if $h(v, \\theta) = 0$ then $v$ would go to the left child and if not, it would go to the right child.\n",
    "    * $I$ is computed using the children and parent sets as the inputs. (children are a function of parent and params)\n",
    "    * For binary classification, best split may occur when the child nodes are as pure as possible (containing training points of a single class). \n",
    "    * Making child nodes and partitioning the dataset into disjoint subsets applies recursively to all the newly constructed nodes after the training begins from the root node.\n",
    "    * The training continues until a stopping criterion is met. The structure of the tree depends on that.\n",
    "    * Stopping criterion options: \n",
    "        * When the three reaches a maximum number of levels \n",
    "        * Impose a minimum value of the information gain at the node (when the sought-for-attributes of the training points within the leaf node are similar)\n",
    "        * When a node contains too few training points\n",
    "    * Avoiding full grown trees (trees where each leaf containing one training point) helps us generalize the models. \n",
    "    * At the end of the training phase, \n",
    "        * The optimum weak learners (split functions) associated with each node\n",
    "        * A learned tree structure \n",
    "        * A different set of training points at each leaf\n",
    "\n",
    "### Weak Learner Models\n",
    "\n",
    "* Important for both training and testing.\n",
    "* The params of the weak learner model is \n",
    "    $$ \\theta = (\\phi, \\psi, \\tau)$$\n",
    "where $\\phi(v)$ selects some features out of $v$, $\\psi$ defines the geometric primitives (e.g. a line) used to separate data, and $\\tau$ captures the threshold for inequalities in the binary test. \n",
    "* These params have to be optimized. \n",
    "\n",
    "#### Linear Data Separation\n",
    "\n",
    "$$ h(v, \\theta) = \\mathbb{I}\\{ \\tau_1 > \\phi(v) \\cdot \\psi > \\tau_2 \\} $$\n",
    "\n",
    "* Axis-aligned weak learned are often used in the boosting literature\n",
    "* They are called \"decision stumps\"\n",
    "* Axis-aligned cased is overparamterized. \n",
    "\n",
    "#### Nonlinear Data Separation\n",
    "\n",
    "* More complex weak learners are obtained by replacing hyperplanes with hier degree of freedom surfaces.\n",
    "* E.g. \n",
    "    $$ h(v, \\theta) = [ \\tau_1 > \\phi(v)^\\top \\psi \\phi(v) > \\tau_2 ]$$\n",
    "* The number of DOFs of the weak learner influences heavily the forest generalization properties. \n",
    "\n",
    "### Energy Models\n",
    "\n",
    "* Basic building blocks of the training objective function are entropy and information gain. \n",
    "* Information gain $I$ at a split node = **reduction in uncertainty** achieved by splitting the data arriving at the node into multiple subsets\n",
    "    $$ I = H(S) - \\sum_{i \\in {L, R}} \\frac{|S^i|}{|S|} H(S^i) $$\n",
    "where $H$ is the entropy (a measure of uncertainty associated with the random variable we wish to predict).\n",
    "* Weighting the entropy using the cardinality of the child sets avoid splitting off childre conraining very few points. \n",
    "* For discrete probability distributions, Shannon entropy can be used: \n",
    "    $$ H(S) = - \\sum_{c \\in C} p(c) \\log p(C) $$\n",
    "where $c$ is the class label and $p(c)$ is the empirical distribution extracted from the training points within training data $S$.\n",
    "* $p(c)$ can be computed as a normalized histogram of the class labels. \n",
    "* When the children distributions are more pure, the entropy decreases and the information content increases. \n",
    "* Maximizing information gain helps us select the split params whcih produces the highest confidence in final distributions. \n",
    "* For continuous probability distributions the differential entropy can be used: \n",
    "    $$ H(S) = - \\int_{y \\in Y} p(y) \\log p(y) dy $$\n",
    "where $y$ is the continuous label and $p(y)$ is the pdf of $y$ estimated from the dataset $S$.\n",
    "* $p(y)$ can be defined using parametric distibutions or non-parametric methods. \n",
    "* Gaussian-based models are frequently used to approximate $p(y)$. \n",
    "* The differential entropy of a $d$-variate Gaussian is,\n",
    "    $$ H(S) = \\frac{1}{2} \\log (2\\pi e)^d | \\Lambda (S) | $$\n",
    "* Overlap between different distributions indicate a suboptimal separation.\n",
    "* The fact that we can define informaiton gain measures for both continuous and discrete distributions, it is useful to build a unified model.\n",
    "\n",
    "### Leaf Prediction Models \n",
    "\n",
    "* Training phase has to learn good prediction models to be stored at the terminal nodes. \n",
    "* In supervising learning, after training, each leaf node has a subset of labels training data. \n",
    "* These can be used to compute label statistics in the leaf to predict the label associated with the input test point. \n",
    "* Generally, leaf statistics can be capture using conditional distributions $p(c|v)$ or $p(y|v)$.\n",
    "* Conditioning denotes that the distributions depend on the specific leaf node reached by the test point. \n",
    "* MAP estimate can be obtained as, \n",
    "    $$ c^* = \\argmax_c p(c|v)$$\n",
    "    $$ y^* = \\argmax_y p(y|v)$$\n",
    "* Don't take ealry point estimates. \n",
    "\n",
    "### The Randomness Model\n",
    "\n",
    "* Randomness is injected to the trees during the training phase\n",
    "    * Random training set sampling\n",
    "    * Randomized node optimization\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "* Reduces overfitting and improves generailization \n",
    "* Trains each tree in a forest on a different training subset sampled at random from the dataset\n",
    "* Avoid specializing the params to a single training set\n",
    "* Training is faster\n",
    "* Not being able to use the entire labeled set is a waste\n",
    "\n",
    "#### Randomized Node Optimization\n",
    "\n",
    "* Node optimiaztion w.r.t the entire parameter space $T$ is inefficient when $T$ is large\n",
    "* Thus, when training node $j$ a small random subset $T_j \\subset T$ is selected and optimized\n",
    "* When $|T| = \\infty$, a parameter $\\rho = |T_j|$ is introduced to control the degree of randomness in tree (usually the value is fixed for all nodes)\n",
    "* In practical applications, we may wish to randomize one, some, or all of the parameters $\\phi, \\psi$ and $\\tau$.\n",
    "* The param values need not be sampled from a uniform distribution.\n",
    "* Bagging and randomized node optimization can be used together\n",
    "\n",
    "### Combining Tree into a Forest Ensemble\n",
    "\n",
    "* A random decision forest is an ensemble of randomly trained decision trees.\n",
    "* The component trees are ranomly different from each other.\n",
    "* This lead to decorrelation between individual tree models and improves generlization and robustness.\n",
    "* The forest model is characterised by,\n",
    "    * family of weak learners\n",
    "    * energy model\n",
    "    * leaf predictors\n",
    "    * type of randomness influence \n",
    "* The randomness parameter $\\rho$ controls, \n",
    "    * amount of randomness between different trees\n",
    "    * amount of correlation between different trees in the forest\n",
    "* In a forest with $T$ trees $t \\in \\{1, \\dots, T \\}$ is used to index the trees.\n",
    "* All the trees are trained indepently and in parallel. \n",
    "* Testing can be done in parallel too.\n",
    "* Combining all the tree predictions into a single forest prediction is done by a simple averaging operation.\n",
    "    $$ p(c|v) = \\frac{1}{T} \\sum_{t=1}^{T} p_t(c|v)$$\n",
    "where $p_t(c|v)$ is the posterior distribution obtained by the $t$ th tree.\n",
    "* Alternatively, \n",
    "    $$ p(c|v) = \\frac{1}{Z} \\prod_{t=1}^{T} p_t(c|v)$$\n",
    "where $Z$ is the partition function that ensures the probabilistic normalization.\n",
    "* Partition function is defined as, \n",
    "    $$ Z = \\sum_{i} \\exp (-\\beta E_i) $$\n",
    "* Peakier distributions are more confident.\n",
    "* The ensembling output is influenced by the most confident, most informative trees. \n",
    "* Averaging many tree posteriors reduce the effect of noisy tree contributions. \n",
    "* The product based tree ensemble model produces sharper distributions and may be less robust to noise. \n",
    "\n",
    "#### Key Model Parameters\n",
    "\n",
    "* the maximum allowed tree depth $D$\n",
    "* the amount of randomness (controlled by $\\rho$) and its type \n",
    "* the forest size $T$\n",
    "* the choice of weak learner model\n",
    "* the training objective function\n",
    "* the choice of features in practical applications.\n",
    "\n",
    "#### Other tips\n",
    "\n",
    "* In general, very unbalanced trees should be avoided. \n",
    "* When training a tree, it is important to visualize its trees and other variables (features and params chose at each node)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}