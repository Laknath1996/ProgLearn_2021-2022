{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction: The Abstract Forest Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction \n",
    "\n",
    "Prototypical ML tasks: \n",
    "* Classification (output is a label)\n",
    "* Regression (output is a continuous variable)\n",
    "* Density estimation (estimating a PDF)\n",
    "* Semi-supervised learning\n",
    "* Active learning (learning using minimum amount of ground-truths)\n",
    "\n",
    "Through a unified model of decision forests, we can map the prototypical ML problems onto the same general model by means of different paramterizations.\n",
    "\n",
    "Decision forests achieve generalization through ensembling slightly different trees.\n",
    "\n",
    "## Decision Tree Basics\n",
    "\n",
    "### Tree Data Structure\n",
    "\n",
    "* A tree is a graph that has a hierachical structure.\n",
    "* Nodes are either internal (split) or terminal (leaf) nodes.\n",
    "* Each node (except the root) have exactly one incoming edge.\n",
    "* A tree doesn't contain loops.\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "* Solving a complex problem by running a series of simpler tests\n",
    "* For a given input, the decision tree estimates an unknown property of the input by asking successive questions about its known properties\n",
    "* The next question depend on the answer to the previous question\n",
    "* The decision is made based on the terminal node\n",
    "* This mechanism can be graphically represented (path followed by the input through the questions)\n",
    "* All the questions asked move our decision making towards the correct region of the decision space\n",
    "* The more question asked, the more higher the confidence in the response\n",
    "* The tests are represented by a decision tree structure. \n",
    "    * Internal nodes: one question/ test each\n",
    "    * Root node: where the input is injected\n",
    "    * Leaf node: predictor (contains the most probable answer based on the questions asked during the tree descent)\n",
    "* Summary: **A decision tree is a tree where each internal node stores a split (or test) function to be applied to the incoming data. Each leaf stores the final answer (predictor). It is a hierachical piecewise model that splits complex problems into simpler ones.** \n",
    "\n",
    "### Mathematical Notations and Basic Defitions\n",
    "\n",
    "#### Data Points and Features\n",
    "\n",
    "* A data point is given by, \n",
    "    $$ \\mathbf{v} = (x_1, x_2, \\dots, x_d) \\in \\mathbb{R}^d $$\n",
    "where $x_i$ is a feature of the data point and $d$ is the feature space dimension (could be infinitely large, but we can pick a subset)\n",
    "* Features of interest are given by,\n",
    "    $$ \\mathbf{\\phi (v)} = (x_{\\phi_1}, x_{\\phi_2}, \\dots, x_{\\phi_{d'}}) \\in \\mathbb{R}^{d'} $$\n",
    "where $d' << d$ and $\\phi_i \\in [1, d]$.\n",
    "\n",
    "#### Test Functions, Split Functions, and Weak Learners\n",
    "\n",
    "* Each node has a different associated test function.\n",
    "* Test function (split function, weak learners) at a split node $j$ is given by, \n",
    "    $$ h(v, \\theta_j): \\mathbb{R}^d \\times \\mathcal{T} \\rightarrow \\{0, 1\\} $$\n",
    "where, $\\theta_j$ are the split parameters of the node $j$.\n",
    "\n",
    "#### Training Points and Training Sets\n",
    "\n",
    "* In a training data point, the attributes we are seeking may be known and could be be used to compute tree parameters.\n",
    "* Definitions: \n",
    "    * $\\mathcal{S}_0$: Training set\n",
    "    * $\\mathcal{S}_j$: Subset of training points reaching node $j$ (nodes are numbered breadth-first)\n",
    "    * $\\mathcal{S}_j^L$: Subset going to the left child of node $j$\n",
    "    * $\\mathcal{S}_j^R$: Subset going to the left child of node $j$\n",
    "* Dataset properties: \n",
    "    * $\\mathcal{S}_j = \\mathcal{S}_j^L \\cup \\mathcal{S}_j^R$\n",
    "    * $\\mathcal{S}_j^L \\cap \\mathcal{S}_j^R =  \\emptyset$\n",
    "    * $\\mathcal{S}_j^L = \\mathcal{S}_{2j+1}$\n",
    "    * $\\mathcal{S}_j^R = \\mathcal{S}_{2j+2}$\n",
    "\n",
    "#### Randomly Trained Decision Trees\n",
    "\n",
    "* **on-line phase: testing**\n",
    "    * The node tests have been selected (by training) and fixed during testing.\n",
    "    * Starting at the root, each split node applies the $h(\\cdot, \\cdot)$ to $\\mathbf{v}$. \n",
    "    * Depending on the result, $\\mathbf{v}$ is sent to the left or right child.\n",
    "    * This is repeated until a leaf node is reached. \n",
    "    * Leaf has a predictor/ estimator (e.g. classifier or a regressor) which assigns a lable to $\\mathbf{v}$.\n",
    "* **off-line phase: training**\n",
    "    * Split functions could be handcrafted / automatically learnt from data.\n",
    "    * Training phase select the type and params of the test function $h(v, \\theta)$ of each split node $j$ by optimizing a chose objective function on an available training set.\n",
    "    * This optimization happens in a greedy manner.\n",
    "    * At each node $j$, based on the incoming training set $\\mathcal{S}_j$, a function is learnt that best splits $\\mathcal{S}_j$ in to $\\mathcal{S}_j^L$ and $\\mathcal{S}_j^R$. This problem is written as, \n",
    "        $$ \\theta_j = \\argmax_{\\theta \\in \\mathcal{T}} I(\\mathcal{S}_j, \\theta) $$\n",
    "    where, $I$ is an objective function.\n",
    "    * This is done by a simple search over a discrete set of samples.\n",
    "    * if $h(v, \\theta) = 0$ then $v$ would go to the left child and if not, it would go to the right child.\n",
    "    * $I$ is computed using the children and parent sets as the inputs. (children are a function of parent and params)\n",
    "    * For binary classification, best split may occur when the child nodes are as pure as possible (containing training points of a single class). \n",
    "    * Making child nodes and partitioning the dataset into disjoint subsets applies recursively to all the newly constructed nodes after the training begins from the root node.\n",
    "    * The training continues until a stopping criterion is met. The structure of the tree depends on that.\n",
    "    * Stopping criterion options: \n",
    "        * When the three reaches a maximum number of levels \n",
    "        * Impose a minimum value of the information gain at the node (when the sought-for-attributes of the training points within the leaf node are similar)\n",
    "        * When a node contains too few training points\n",
    "    * Avoiding full grown trees (trees where each leaf containing one training point) helps us generalize the models. \n",
    "    * At the end of the training phase, \n",
    "        * The optimum weak learners (split functions) associated with each node\n",
    "        * A learned tree structure \n",
    "        * A different set of training points at each leaf\n",
    "\n",
    "### Weak Learner Models\n",
    "\n",
    "* Important for both training and testing.\n",
    "* The params of the weak learner model is \n",
    "    $$ \\theta = (\\phi, \\psi, \\tau)$$\n",
    "where $\\phi(v)$ selects some features out of $v$, $\\psi$ defines the geometric primitives (e.g. a line) used to separate data, and $\\tau$ captures the threshold for inequalities in the binary test. \n",
    "* These params have to be optimized. \n",
    "\n",
    "#### Linear Data Separation\n",
    "\n",
    "$$ h(v, \\theta) = \\mathbb{I}\\{ \\tau_1 > \\phi(v) \\cdot \\psi > \\tau_2 \\} $$\n",
    "\n",
    "* Axis-aligned weak learned are often used in the boosting literature\n",
    "* They are called \"decision stumps\"\n",
    "* Axis-aligned cased is overparamterized. \n",
    "\n",
    "#### Nonlinear Data Separation\n",
    "\n",
    "* More complex weak learners are obtained by replacing hyperplanes with hier degree of freedom surfaces.\n",
    "* E.g. \n",
    "    $$ h(v, \\theta) = [ \\tau_1 > \\phi(v)^\\top \\psi \\phi(v) > \\tau_2 ]$$\n",
    "* The number of DOFs of the weak learner influences heavily the forest generalization properties. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}